{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "In this notebook we will learn to build an Artificial Neural Network, based on the training data generated by the notebook number 2. But...\n",
    "\n",
    "\n",
    "What is an Artificial Neural Network? \n",
    "\n",
    "#### The Perceptron\n",
    "\n",
    "Before giving the definition of a neural network, we need to take a step back and establish what is meant by Artificial Neuron (or **Perceptron**). \n",
    "\n",
    "An Artificial Neuron is a function that maps an input vector $\\{x_1, ..., x_k\\}$ to a scalar output $y$ via a weight\n",
    "vector $\\{w_1, ..., w_k\\}$ and a function $f$ (typically non-linear).\n",
    "<img src=\"images/neuron.png\">\n",
    "$$y = \\sum_{i=0}^{k} w_ix_i = f(w^Tx)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The Activation Function\n",
    "The function $f$ is called the activation function and generates a non-linear input/output relationship. A common choice for the activation function is the **Logistic function** (or **Sigmoid**).\n",
    "<img src=\"images/activation.png\" height=\"500\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the weights: an optimization problem\n",
    "\n",
    "We want to find the weights $\\{w_1, ..., w_k\\}$ such that the **objective function** (or **loss**) is minimized. The objective function  measures the difference between the actual output $t$ and the predicted output $y$.\n",
    "\n",
    "To find the weights, we will use the **Gradient Descent**:\n",
    "- Iterative optimization algorithm used in machine learning to find the best results (minima of a curve).\n",
    "- Compute the gradient of the objective function with respect to an element $w_i$ of the vector $\\{w_1, ..., w_k\\}$.\n",
    "<img src=\"images/gradient.png\" height=\"500\" width=\"500\">\n",
    "\n",
    "- Let’s update the weights using the gradient descent update equation (in vector notation):\n",
    "\n",
    "$$w^{new}_i = w^{old}_i - \\eta \\frac{\\partial J(w)}{\\partial w_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  The Hyperparameters of an Artificial Neural Network\n",
    "\n",
    "Hyperparameters are the parameters which determine the **network structure** (e.g. Number of Hidden Units) and the parameters which determine **how the network is trained** (e.g. Learning Rate).\n",
    "\n",
    "1. **Learning Rate $\\eta$**\n",
    "    \n",
    "    - Training parameter that controls the size of weight changes in the learning phase of the training algorithm.\n",
    "    - The learning rate determines how much an updating step influences the current value of the weights.\n",
    "\n",
    "*Many updates required before reaching the minimum*. \n",
    "\n",
    "*Drastic updates can lead to divergent behaviors, missing the minimum*.\n",
    "\n",
    "<img src=\"images/small.png\" height=\"250\" width=\"200\">\n",
    "<img src=\"images/big.png\" height=\"250\" width=\"250\"> \n",
    "\n",
    "2. **Momentum $\\alpha$**\n",
    "    - Momentum simply adds a fraction of the previous weight update to the current one.\n",
    "    - When the gradient keeps pointing in the same direction, this will increase the size of the steps taken towards the minimum.\n",
    "    \n",
    "    <img src=\"images/momentum.png\" height=\"250\" width=\"250\"> \n",
    "    $$\\Delta w_i(t+1) = - \\eta \\frac{\\partial J(w)}{\\partial w_i} + \\alpha \\Delta w_i(t) $$\n",
    "\n",
    "3. **Weight decay $\\lambda$**\n",
    "     - Weight decay $\\lambda>0$ penalizes the weight changes.\n",
    "     - By shrinking your coefficients toward zero, it tends to decrease the magnitude of the weights, and helps prevent overfitting\n",
    "     $$\\Delta w_i(t+1) = - \\eta \\frac{\\partial J(w)}{\\partial w_i} - \\lambda \\eta w_i(t) $$\n",
    "\n",
    "4. **Number of epochs**\n",
    "    - The number of epochs is the number of times the whole training data is shown to the network while training.\n",
    "    \n",
    "5. **Batch size**\n",
    "   - The number of samples shown to the network before the gradient computation and the parameter update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the knowledge base\n",
    "First of all, you need to load the knowledge base, ie the training data contained in one of the files generated in the previous notebook. Use `m`, `N` and `num_of_matches` to load the right model\n",
    "\n",
    "To do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# These parameters must be set to load the correct training set\n",
    "\n",
    "m = 1\n",
    "N = 10\n",
    "num_of_matches = 10\n",
    "\n",
    "path = 'output/train_set_m{}/num_of_matches_{}.txt'.format(m, num_of_matches)\n",
    "dataset = pandas.read_csv(path, ',', delimiter=None, header=None)\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset: \" + path + '\\n')\n",
    "print(dataset)\n",
    "print(\"\\nx:\")\n",
    "print(X)\n",
    "print(\"\\ny:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Label Encoding and One Hot Encoding!\n",
    "Label Encoder is used to convert categorical data, or text data, into numbers, which our predictive models can better understand. What one hot encoding does is, it takes a column which has categorical data, which has been label encoded, and then splits the column into multiple columns. The numbers are replaced by 1s and 0s, depending on which column has what value.\n",
    "\n",
    "This is to ensure that each example has an expected probability of 1.0 for the actual class value and an expected probability of 0.0 for all other class values when `softmax` activation function is used. This can be achieved using the `to_categorical()` Keras function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "y_tc = to_categorical(encoded_y, 4)\n",
    "print(y)\n",
    "print(\"is converted into\")\n",
    "print(encoded_y)\n",
    "print(\"\\n one hot encoding\")\n",
    "print(y_tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Split your data!\n",
    "All you have to do is divide your training data into **training set** and **test set** because later we want to evaluate our classifier's performance.\n",
    "\n",
    "To do is invoke these simple commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_tc, test_size=0.3, random_state=4)\n",
    "\n",
    "# print the shapes of the new X objects\n",
    "print(\"\\nTraining set dimensions (X_train):\")\n",
    "print(X_train.shape)\n",
    "print(\"\\nTest set dimensions (X_test):\")\n",
    "print(X_test.shape)\n",
    "\n",
    "# print the shapes of the new y objects\n",
    "print(\"\\nTraining set dimensions (y_train):\")\n",
    "print(y_train.shape)\n",
    "print(\"\\nTest set dimensions (y_test):\")\n",
    "print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling data\n",
    "Many machine learning algorithms require that features are on the same scale. Also, optimization algorithms such as gradient descent work best if our features are centered at mean zero with a standard deviation of one — i.e., the data has the properties of a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Define the scaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "# Scale the training set\n",
    "X_train = scaler.transform(X_train)\n",
    "# Scale the test set\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Build the Neural Network Architecture\n",
    "\n",
    "Now we are ready to build our classifier: the core data structure of Keras is a model, a way to organize layers. The simplest type of model is the **Sequential model**, a linear stack of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking layers is as easy as `.add()`. Activations can either be used through an Activation layer, or through the `activation` argument supported by all forward layers.\n",
    "\n",
    "##### Avoid overfitting with Dropout\n",
    "\n",
    "Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel.\n",
    "\n",
    "During training, some number of layer outputs are randomly ignored or “dropped out.” This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. In effect, each update to a layer during training is performed with a different “view” of the configured layer. \n",
    "\n",
    "Dropout has the effect of making the training process noisy, forcing nodes within a layer to probabilistically take on more or less responsibility for the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense, Dropout\n",
    "\n",
    "model.add(Dense(45, input_shape=(X_train.shape[1],)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(30))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(15))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your model looks good, configure its learning process with `.compile()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "adm = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adm,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now iterate on your training data in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "          epochs=200,\n",
    "          batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Evaluate the Classifier\n",
    "This phase is very important and allows us to evaluate the model based on some standard metrics, such as **accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Make predictions for the labels of the test set\n",
    "y_pred = model.predict_classes(X_test)\n",
    "print(\"\\nPredictions\")\n",
    "print(y_pred)\n",
    "\n",
    "# Evaulate model\n",
    "score = model.evaluate(X_test, y_test,verbose=1)\n",
    "# The score is a list that holds the loss and the accuracy\n",
    "print(\"\\nScore (loss,accuracy)\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  The Loss plot\n",
    "\n",
    "This graph allow us to understand whether our model is overfitting or not. In fact, one way to limit this phenomenon is to set the number of training periods to such a value that the loss of the validation set begins to increase, which is a symptom of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(5.0, 5.0), dpi=120)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(history.epoch, np.array(history.history['loss']),\n",
    "           label='Train Loss')\n",
    "plt.plot(history.epoch, np.array(history.history['val_loss']),\n",
    "           label = 'Val loss')\n",
    "plt.legend()\n",
    "plt.ylim([0, 2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  The Accuracy plot\n",
    "\n",
    "This graph can provide an indication of useful things about the training of the model, such as:\n",
    "\n",
    "   - It’s speed of convergence over epochs (slope).\n",
    "   - Whether the model may have already converged (plateau of the line).\n",
    "   - Whether the mode may be over-learning the training data (inflection for validation line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5.0, 5.0), dpi=120)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accurancy')\n",
    "plt.plot(history.epoch, np.array(history.history['acc']),\n",
    "           label='Train Acc')\n",
    "plt.plot(history.epoch, np.array(history.history['val_acc']),\n",
    "           label = 'Val Acc')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameters Tuning\n",
    "\n",
    "Hyperparameter optimization is a big part of deep learning.\n",
    "\n",
    "The reason is that neural networks are notoriously difficult to configure and there are a lot of parameters that need to be set. On top of that, individual models can be very slow to train.\n",
    "\n",
    "\n",
    "In the next optional notebook it is possible to retrace the steps performed in this notebook, but with a different approach: we will try to perform hyperparameter tuning using Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save your model\n",
    "\n",
    "You have reached the last step of this notebook. If the developed model satisfies you, all you have to do is save it in the same folder that contains the training data. This is important because this model will be loaded into the next notebook. Therefore good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_of_model = 'output/train_set_m{}/model_N_N_{}_{}.h5'.format(m, num_of_matches, num_of_matches)\n",
    "model.save(path_of_model)\n",
    "\n",
    "print('model saved in {}'.format(path_of_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"images/ultron.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
