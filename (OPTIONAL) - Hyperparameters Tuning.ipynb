{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Tuning\n",
    "\n",
    "In this notebook i want to show you both how you can use the Scikit-learn grid search capability and give you an example that you can copy-and-paste into your own project as a starting point.\n",
    "\n",
    "\n",
    "We will not go into the details of some passages since they have already been proposed several times in previous notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the knowledge base\n",
    "First of all, you need to load the knowledge base, ie the training data contained in one of the files generated in the previous notebook. Use `m`, `N` and `num_of_matches` to load the right file.\n",
    "\n",
    "To do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# These parameters must be set to load the correct training set\n",
    "\n",
    "m = 1\n",
    "N = 10\n",
    "num_of_matches = 10\n",
    "\n",
    "path = 'output/train_set_m{}/num_of_matches_{}.txt'.format(m, num_of_matches)\n",
    "dataset = pandas.read_csv(path, ',', delimiter=None, header=None)\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "\n",
    "\n",
    "print(\"Dataset: \" + path + '\\n')\n",
    "print(dataset)\n",
    "print(\"\\nx:\")\n",
    "print(X)\n",
    "print(\"\\ny:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Label Encoding and One Hot Encoding!\n",
    "Label Encoder is used to convert categorical data, or text data, into numbers, which our predictive models can better understand. What one hot encoding does is, it takes a column which has categorical data, which has been label encoded, and then splits the column into multiple columns. The numbers are replaced by 1s and 0s, depending on which column has what value.\n",
    "\n",
    "This is to ensure that each example has an expected probability of 1.0 for the actual class value and an expected probability of 0.0 for all other class values when `softmax` activation function is used. This can be achieved using the `to_categorical()` Keras function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "y_tc = to_categorical(encoded_y, 4)\n",
    "print(y)\n",
    "print(\"is converted into\")\n",
    "print(encoded_y)\n",
    "print(\"\\n one hot encoding\")\n",
    "print(y_tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Split your data!\n",
    "All you have to do is divide your training data into **training set** and **test set** because later we want to evaluate our classifier's performance.\n",
    "\n",
    "To do is invoke these simple commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_tc, test_size=0.3, random_state=4)\n",
    "\n",
    "# print the shapes of the new X objects\n",
    "print(\"\\nTraining set dimensions (X_train):\")\n",
    "print(X_train.shape)\n",
    "print(\"\\nTest set dimensions (X_test):\")\n",
    "print(X_test.shape)\n",
    "\n",
    "# print the shapes of the new y objects\n",
    "print(\"\\nTraining set dimensions (y_train):\")\n",
    "print(y_train.shape)\n",
    "print(\"\\nTest set dimensions (y_test):\")\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling data\n",
    "Many machine learning algorithms require that features are on the same scale. Also, optimization algorithms such as gradient descent work best if our features are centered at mean zero with a standard deviation of one â€” i.e., the data has the properties of a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Define the scaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "# Scale the training set\n",
    "X_train = scaler.transform(X_train)\n",
    "# Scale the test set\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Keras Models in scikit-learn\n",
    "\n",
    "Keras models can be used in scikit-learn by wrapping them with the `KerasClassifier` or `KerasRegressor` class.\n",
    "\n",
    "To use these wrappers you must define a function that creates and returns your Keras sequential model, then pass this function to the `build_fn` argument when constructing the KerasClassifier class.\n",
    "\n",
    "The hyperparameters we want to search must appear as formal parameters of `create_model`function.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "def create_model(layers, activation, dropout_rate):\n",
    "    model = Sequential()\n",
    "    for i, nodes in enumerate(layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(nodes, input_dim=X_train.shape[1]))\n",
    "            model.add(Activation(activation))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        else:\n",
    "            model.add(Dense(nodes))\n",
    "            model.add(Activation(activation))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(4))  # Note: no activation beyond this point\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a dictionary of hyperparameters\n",
    "\n",
    "As you will already have understood, the hyperparameters that want to look for are the number of hidden layers, the activation function of each and the dropout rate.\n",
    "\n",
    "\n",
    "You can search for any hyperparameter, the only rule is that you must specify the dictionary that contains them all!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [[45, 30, 15]]\n",
    "activations = ['sigmoid', 'relu', 'elu']\n",
    "dropout_rate = [0.0, 0.1]\n",
    "param_grid = dict(layers=layers, activation=activations, batch_size = [60,128, 256], epochs=[200], dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The GridSearchCV class\n",
    "\n",
    "Grid search is a model hyperparameter optimization technique.\n",
    "\n",
    "In scikit-learn this technique is provided in the `GridSearchCV` class.\n",
    "\n",
    "When constructing this class you must provide a dictionary of hyperparameters to evaluate in the param_grid argument. This is a map of the model parameter name and an array of values to try.\n",
    "\n",
    "By default, accuracy is the score that is optimized, but other scores can be specified in the score argument of the GridSearchCV constructor.\n",
    "\n",
    "By default, the grid search will only use one thread. By setting the n_jobs argument in the GridSearchCV constructor to -1, the process will use all cores on your machine. Depending on your Keras backend, this may interfere with the main neural network training process.\n",
    "\n",
    "The GridSearchCV process will then construct and evaluate one model for each combination of parameters. Cross validation is used to evaluate each individual model and the default of 3-fold cross validation is used, although this can be overridden by specifying the cv argument to the GridSearchCV constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once completed, you can access the outcome of the grid search in the result object returned from `grid.fit()`. The`best_score_` member provides access to the best score observed during the optimization procedure and the`best_params_` describes the combination of parameters that achieved the best results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
